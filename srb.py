
import random
import os
import h5py
import zarr
import sys
import pandas as pd
import daiquiri
#import bsddb3
import time
import scipy
import pickle
import collections
import itertools
import operator
import tqdm
import shutil
import pprint
import numpy as np
import json

import matplotlib as mp
# Force matplotlib to not use any Xwindows backend.
mp.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

import tsinfer
import msprime


old_simplify = msprime.TreeSequence.simplify
def new_simplify(ts, *args, **kwargs):
    tmp_tables = ts.dump_tables()
    tmp_tables.nodes.clear()
    tmp_tables.individuals.clear()
    for node in ts.nodes():
        if node.is_sample():
            try:
                metadata = ts.individual(node.individual).metadata
            except:
                metadata = None
        else:
            metadata = None
        tmp_tables.nodes.add_row(
            flags=node.flags, time=node.time, metadata=metadata)
    tmp_tables.nodes.set_columns(
       flags=tmp_tables.nodes.flags,
       time=tmp_tables.nodes.time,
       metadata=tmp_tables.nodes.metadata,
       metadata_offset=tmp_tables.nodes.metadata_offset)
    return old_simplify(tmp_tables.tree_sequence(), *args, **kwargs)

msprime.TreeSequence.simplify = new_simplify


def make_errors(v, p):
    """
    For each sample an error occurs with probability p. Errors are generated by
    sampling values from the stationary distribution, that is, if we have an
    allele frequency of f, a 1 is emitted with probability f and a
    0 with probability 1 - f. Thus, there is a possibility that an 'error'
    will in fact result in the same value.
    """
    w = np.copy(v)
    if p > 0:
        m = v.shape[0]
        frequency = np.sum(v) / m
        # Randomly choose samples with probability p
        samples = np.where(np.random.random(m) < p)[0]
        # Generate observations from the stationary distribution.
        errors = (np.random.random(samples.shape[0]) < frequency).astype(int)
        w[samples] = errors
    return w


def generate_samples(ts, error_p):
    """
    Returns samples with a bits flipped with a specified probability.

    Rejects any variants that result in a fixed column.
    """
    S = np.zeros((ts.sample_size, ts.num_mutations), dtype=np.int8)
    for variant in ts.variants():
        done = False
        # Reject any columns that have no 1s or no zeros
        while not done:
            S[:, variant.index] = make_errors(variant.genotypes, error_p)
            s = np.sum(S[:, variant.index])
            done = 0 < s < ts.sample_size
    return S.T

Edge_to_parent = collections.namedtuple("Edge_to_parent", "parent_id left right")

def identify_SRBs(ts):
    SRB_data = collections.namedtuple("SRB_data", "pos left_parent right_parent")
    breakpoints = collections.defaultdict(set)
    for (left, right), edges_out, edges_in in ts.edge_diffs():
        if len(edges_out) and len(edges_in):
            child_data = collections.defaultdict(list)
            for edge in edges_out:
                child_data[edge.child].append(edge.parent)
            for edge in edges_in:
                child_data[edge.child].append(edge.parent)
            for c, parents in child_data.items():
                assert 0 < len(parents) < 3
                if len(parents) == 2:
                    breakpoints[SRB_data(left, parents[0],parents[1])].add(c)
    # shared breakpoints have at least 2 children
    return {k:v for k,v in breakpoints.items() if len(v) > 1}


def SRB_replace_edges(new_id, time, SRB, SRB_children, child_to_parent):
    """
    Make a new node and insert it as a replacement into the child_to_parent list
    Return a tuple of the number of edges deleted and created
    NB: to avoid collision with existing sample nodes, we should refer to these
    newly created nodes by a *negative* number
    """
    # find leftmost span of l parent and rightmost of r parent
    # this assumes contiguous edges on a parent
    for p in (SRB.left_parent, SRB.right_parent):
        for i in range(len(child_to_parent[p])-1):
            assert child_to_parent[p][i].right == child_to_parent[p][i+1].left
    l_parent_lft = child_to_parent[SRB.left_parent][0].left 
    r_parent_rgt = child_to_parent[SRB.right_parent][-1].right
    #insert as replacement
    assert new_id != SRB.left_parent
    assert new_id != SRB.right_parent
    child_to_parent[new_id] = [
        Edge_to_parent(parent_id=SRB.left_parent, left=l_parent_lft, right=SRB.pos),
        Edge_to_parent(parent_id=SRB.right_parent, left=SRB.pos, right=r_parent_rgt)]
    edges_inserted = 2
    
    # relabel existing children to point to this new parent either side of the
    # breakpoint
    edges_deleted = 0
    for child in SRB_children:
        #print(" SRB in child {}".format(child))
        found = False
        edges = child_to_parent[child]
        i=0
        while i < (len(edges)-1):
            assert new_id != child
            if (edges[i].right == SRB.pos and 
                edges[i+1].left == SRB.pos): #we know this must be shared
                
                print("Inserting single edge [{},{}) from child {} to parent {}".format(
                    edges[i].left, edges[i+1].right, child, new_id))
                edges[i] = Edge_to_parent(
                    new_id, edges[i].left, edges[i+1].right)
                del edges[i+1]
                edges_deleted += 1
                found = True
            i+=1
        if not found:
            print("Cannot find breakpoint in child {}".format(child))
            print("Should be at {}".format(child))
            assert False
    return edges_inserted, edges_deleted

def tsinfer_dev(
        n, L, seed, num_threads=1, recombination_rate=1e-8,
        error_rate=0, engine="C", log_level="WARNING",
        debug=True, progress=False, path_compression=True):

    np.random.seed(seed)
    random.seed(seed)
    L_megabases = int(L * 10**6)

    # daiquiri.setup(level=log_level)

    ts = msprime.simulate(
            n, Ne=10**4, length=L_megabases,
            recombination_rate=recombination_rate, mutation_rate=1e-8,
            random_seed=seed)
    if debug:
        print("num_sites = ", ts.num_sites)
    assert ts.num_sites > 0

    use_built_in_path_compression = True

    sample_data = tsinfer.SampleData.from_tree_sequence(ts)

    ancestor_data = tsinfer.generate_ancestors(
        sample_data, engine=engine, num_threads=num_threads)
    ancestors_ts = tsinfer.match_ancestors(
        sample_data, ancestor_data, engine=engine, 
        path_compression=True, extended_checks=True)

    # Do not simplify the ancestors, since (1) we will not be able to map parents
    #  back to ancestors in the original ancestors_ts, and (2) we may lose where in the 
    #  timeslices to put the new ancestors. OTOH, we may miss some SRBs, if parents are 
    #  different between children
    full_ts = tsinfer.match_samples(
        sample_data, ancestors_ts, engine=engine, simplify=False,
        path_compression=use_built_in_path_compression, extended_checks=True)

    SRBs = identify_SRBs(full_ts)
    print("SRB count at start")
    # print how many SRBs we have
    ct = np.array([len(bp) for bp in SRBs.values()], dtype=np.int)
    edgecount = full_ts.num_edges, full_ts.simplify().num_edges
    print(
        ", ".join(["{}:{}".format(i,n) for i,n in enumerate(np.bincount(ct)) if i>1 and n>0])
        + " -- Number of edges in ts = {} ({} simplified)".format(*edgecount))
    # sort so that we hit the most frequent SRBs first
    sorted_SRB_keys = sorted(list(SRBs.keys()), key=lambda x:len(SRBs[x]), reverse=True)
    
    for it_target in [38]:
        tables = full_ts.dump_tables()
        samples = set()
        # make a new structure indexed by child rather than by parent
        # we need to keep track of samples too, as the parents of samples need to be
        # passed up
        child_to_parent = collections.defaultdict(list)
        for e in tables.edges:
            child_to_parent[e.child].append(Edge_to_parent(e.parent,e.left, e.right))
        for child, arr in child_to_parent.items():
            arr.sort(key=operator.attrgetter('left'))
        for i, n in enumerate(tables.nodes):
            if n.flags & msprime.NODE_IS_SAMPLE:
                samples.add(i)       
        print("Samples", samples)
        
        tables = ancestors_ts.dump_tables()
        internal_node_times = {i:n.time for i, n in enumerate(tables.nodes)}
    
        #allocate a new ancestor for every shared breakpoint site        
        delta=0.0000000001 #must be smaller than the delt used in path compression
        edges_differences = np.zeros(2, dtype=np.int) #count inserted, deleted
        for it, SRB in enumerate(sorted_SRB_keys):
            youngest_parent_time = min(
                internal_node_times[SRB.left_parent],
                internal_node_times[SRB.right_parent])
            new_time = youngest_parent_time-delta
            new_id = -tables.nodes.add_row(time=new_time, flags=tsinfer.SYNTHETIC_NODE_BIT)
            internal_node_times[new_id] = new_time
            print("Inserted new node", new_id, "@", new_time)
            SRB_children = SRBs[SRB]
            edges_differences += SRB_replace_edges(
                new_id, youngest_parent_time, SRB, SRB_children, child_to_parent)
            
            if it == it_target:
                print(SRB, SRB_children)
                break
                            
        # remake the ancestors table, but don't use the samples: we will match them up later
        tables.edges.clear()
        for c, edges in child_to_parent.items():
            if c in samples:
                #print("omitting edges from child {}".format(c))
                pass
            else:
                #print("Saving edges from child {}".format(c))
            
                for e in edges:
                    try:
                        assert e.parent_id != c
                        assert internal_node_times[e.parent_id] > internal_node_times[c]
                    except:
                        print(
                            "Parent {}@{}, child {}@{}".format(e.parent_id,
                                internal_node_times[e.parent_id], c, internal_node_times[c]))
                        raise
                    tables.edges.add_row(
                        left=e.left, right=e.right, parent=abs(e.parent_id), child=abs(c))
        
        tables.sort()
        print("{} edges inserted / deleted".format(edges_differences))

        ancestors_ts2 = tables.tree_sequence()

        print("new ancestors tree seq made")    


        child_to_parent = collections.defaultdict(list)
        for e in tables.edges:
            child_to_parent[e.child].append(Edge_to_parent(e.parent,e.left, e.right))
        for child, arr in child_to_parent.items():
            arr.sort(key=operator.attrgetter('left'))
        for i, n in enumerate(tables.nodes):
            if n.flags & msprime.NODE_IS_SAMPLE:
                samples.add(i)       
        internal_node_times = {i:n.time for i, n in enumerate(tables.nodes)}

        SRB = sorted_SRB_keys[39]
        youngest_parent_time = min(
            internal_node_times[SRB.left_parent],
            internal_node_times[SRB.right_parent])
        new_time = youngest_parent_time-delta
        new_id = -tables.nodes.add_row(time=new_time, flags=tsinfer.SYNTHETIC_NODE_BIT)
        internal_node_times[new_id] = new_time
        print("Inserted new node", new_id, "@", new_time, 
        "( parents:", SRB.left_parent, SRB.right_parent, ")")
        
        SRB_children = SRBs[SRB]
        print("Children at times", {c:internal_node_times[c] for c in SRB_children})
        edges_differences += SRB_replace_edges(
            new_id, youngest_parent_time, SRB, SRB_children, child_to_parent)

        
        # remake the ancestors table, but don't use the samples: we will match them up later
        tables.edges.clear()
        for c, edges in child_to_parent.items():
            if c in samples:
                print("omitting edges from child {}".format(c))
                pass
            else:
                print("Saving edges from child {}".format(c))
            
                for e in edges:
                    try:
                        assert e.parent_id != c
                        assert internal_node_times[e.parent_id] > internal_node_times[c]
                    except:
                        print(
                            "Parent {}@{}, child {}@{}".format(e.parent_id,
                                internal_node_times[e.parent_id], c, internal_node_times[c]))
                        raise
                    tables.edges.add_row(
                        left=e.left, right=e.right, parent=abs(e.parent_id), child=abs(c))

        tables.sort()

        ancestors_ts2 = tables.tree_sequence()

        print("new ancestors tree seq made")    
        full_ts2 = tsinfer.match_samples(
            sample_data, ancestors_ts2, engine=engine, simplify=False,
            path_compression=use_built_in_path_compression)


    

    SRBs = identify_SRBs(full_ts)
    print("SRB count at end")
    # print how many SRBs we have
    ct = np.array([len(bp) for bp in SRBs.values()], dtype=np.int)
    post_edgecount = full_ts.num_edges, full_ts.simplify().num_edges
    print(
        ", ".join(["{}:{}".format(i,n) for i,n in enumerate(np.bincount(ct)) if i>1])
        + " -- Number of edges in ts = {} ({} simplified)".format(*post_edgecount))

    for c, pre, post in zip(["without", "with"], edgecount, post_edgecount):
        print("Reduction in edges {} simplification: {}".format(c, (pre-post)/pre*100))

    for node in ts.nodes():
        if tsinfer.is_synthetic(node.flags):
            print("Synthetic node", node.id, node.time)
            parent_edges = [edge for edge in ts.edges() if edge.parent == node.id]
            child_edges = [edge for edge in ts.edges() if edge.child == node.id]
            child_edges.sort(key=lambda e: e.left)
            print("parent edges")
            for edge in parent_edges:
                print("\t", edge)
            print("child edges")
            for edge in child_edges:
                print("\t", edge)

def subset_sites(ts, position):
    """
    Return a copy of the specified tree sequence with sites reduced to those
    with positions in the specified list.
    """
    tables = ts.dump_tables()
    lookup = frozenset(position)
    tables.sites.clear()
    tables.mutations.clear()
    for site in ts.sites():
        if site.position in lookup:
            site_id = tables.sites.add_row(
                site.position, ancestral_state=site.ancestral_state,
                metadata=site.metadata)
            for mutation in site.mutations:
                tables.mutations.add_row(
                    site_id, node=mutation.node, parent=mutation.parent,
                    derived_state=mutation.derived_state,
                    metadata=mutation.metadata)
    return tables.tree_sequence()

def minimise(ts):
    tables = ts.dump_tables()

    out_map = {}
    in_map = {}
    first_site = 0
    for (_, edges_out, edges_in), tree in zip(ts.edge_diffs(), ts.trees()):
        for edge in edges_out:
            out_map[edge.child] = edge
        for edge in edges_in:
            in_map[edge.child] = edge
        if tree.num_sites > 0:
            sites = list(tree.sites())
            if first_site:
                x = 0
                first_site = False
            else:
                x = sites[0].position
            print("X = ", x)
            for edge in out_map.values():
                print("FLUSH", edge)
            for edge in in_map.values():
                print("INSER", edge)

            # # Flush the edge buffer.
            # for left, parent, child in edge_buffer:
            #     tables.edges.add_row(left, x, parent, child)
            # # Add edges for each node in the tree.
            # edge_buffer.clear()
            # for root in tree.roots:
            #     for u in tree.nodes(root):
            #         if u != root:
            #             edge_buffer.append((x, tree.parent(u), u))

    # position = np.hstack([[0], tables.sites.position, [ts.sequence_length]])
    # position = tables.sites.position
    # edges = []
    # print(position)
    # tables.edges.clear()
    # for edge in ts.edges():
    #     left = np.searchsorted(position, edge.left)
    #     right = np.searchsorted(position, edge.right)

    #     print(edge, left, right)
    #     # if right - left > 1:
    #         # print("KEEP:", edge, left, right)
    #         # tables.edges.add_row(
    #         #     position[left], position[right], edge.parent, edge.child)
    #         # print("added", tables.edges[-1])
    #     # else:
    #         # print("SKIP:", edge, left, right)

    # ts = tables.tree_sequence()
    # for tree in ts.trees():
    #     print("TREE:", tree.interval)
    #     print(tree.draw(format="unicode"))





def minimise_dev():
    ts = msprime.simulate(5, mutation_rate=1, recombination_rate=2, random_seed=3)
    # ts = msprime.load(sys.argv[1])

    position = ts.tables.sites.position[::2]
    subset_ts = subset_sites(ts, position)
    print("Got subset")

    ts_new = tsinfer.minimise(subset_ts)
    for tree in ts_new.trees():
        print("TREE:", tree.interval)
        print(tree.draw(format="unicode"))
    # print(ts_new.tables)
    print("done")
    other = minimise(subset_ts)


def run_build():

    sample_data = tsinfer.load(sys.argv[1])
    ad = tsinfer.generate_ancestors(sample_data)
    print(ad)


if __name__ == "__main__":

    # run_build()

    # np.set_printoptions(linewidth=20000)
    # np.set_printoptions(threshold=20000000)

    # tutorial_samples()

    # build_profile_inputs(10, 10)
    # build_profile_inputs(100, 10)
    # build_profile_inputs(1000, 100)
    # build_profile_inputs(10**4, 100)
    # build_profile_inputs(10**5, 100)

    # for j in range(1, 100):
    #     tsinfer_dev(15, 0.5, seed=j, num_threads=0, engine="P", recombination_rate=1e-8)
    # copy_1kg()
    tsinfer_dev(5, 2, seed=4, num_threads=0, engine="C", recombination_rate=1e-8)
    #tsinfer_dev(4, 0.5, seed=423, num_threads=0, engine="C", recombination_rate=1e-8)
    #tsinfer_dev(4, 0.5, seed=2345, num_threads=0, engine="C", recombination_rate=1e-8)
    #tsinfer_dev(10, 2, seed=689, num_threads=0, engine="C", recombination_rate=1e-8)

    # minimise_dev()

#     for seed in range(1, 10000):
#         print(seed)
#         # tsinfer_dev(40, 2.5, seed=seed, num_threads=1, genotype_quality=1e-3, engine="C")
